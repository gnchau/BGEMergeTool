{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import hail as hl\n",
    "from datetime import date\n",
    "import numpy as np\n",
    "from bokeh.plotting import figure, show\n",
    "from bokeh.io import output_notebook\n",
    "from hail.fs.hadoop_fs import hadoop_open\n",
    "\n",
    "\n",
    "class JointCalledSet:\n",
    "    \"\"\"\n",
    "    Represents a joint set of genetic variant calls obtained from exome sequencing (exo)\n",
    "    and imputation (imp) datasets. Provides methods for loading, merging, and analyzing these datasets.\n",
    "\n",
    "    Args:\n",
    "        exo_path (str): Path to the exome variant call file in VCF format.\n",
    "        imp_path (str): Path to the imputation variant call file in VCF format.\n",
    "        output_directory (str): Directory where the output files will be saved.\n",
    "        priority (str, optional): The priority dataset ('exome' or 'imputation'). Defaults to 'exome'.\n",
    "\n",
    "    Attributes:\n",
    "        exo (hl.MatrixTable): Hail MatrixTable representing the exome dataset.\n",
    "        imp (hl.MatrixTable): Hail MatrixTable representing the imputation dataset.\n",
    "        out_path_prefix (str): Prefix for the output file paths.\n",
    "        priority (str): The priority dataset ('exome' or 'imputation').\n",
    "        merged (hl.MatrixTable): Merged dataset with priority given to the specified dataset.\n",
    "        overlap (hl.MatrixTable): Dataset representing the overlap of variants and samples between exome and imputation datasets.\n",
    "    \"\"\"\n",
    "    def __init__(self, \n",
    "                 exo_path, \n",
    "                 imp_path, \n",
    "                 output_directory, \n",
    "                 log_outname=f'merging-tool-{date.today().isoformat()}-{np.random.randint(100,999)}.log',\n",
    "                 priority='exome',\n",
    "                 keep_all_fields=False,\n",
    "                 impute_nonsimilar_samples=False):\n",
    "        self.exo_path, self.imp_path = exo_path, imp_path\n",
    "        self.load_sets(exo_path, imp_path)\n",
    "        self.log = ''\n",
    "        _msg = ''\n",
    "                \n",
    "        if output_directory[-1] != '/':\n",
    "            raise ValueError('Output Directory must be a directory!')\n",
    "        else:\n",
    "            self.out_path_prefix = output_directory\n",
    "\n",
    "        self.log_outpath = f'{self.out_path_prefix}{log_outname}'\n",
    "        print(f'Logging session at {self.log_outpath}.')\n",
    "\n",
    "        self.exo = self.exo.annotate_entries(\n",
    "            GT = hl.vds.lgt_to_gt(self.exo.LGT, self.exo.LA)\n",
    "        )\n",
    "\n",
    "        self.priority = priority\n",
    "        \n",
    "        if not keep_all_fields:\n",
    "            # drop these but we might want them later\n",
    "            fields_to_remove_exo = list(self.exo.entry.dtype.fields)\n",
    "            fields_to_remove_exo.append('info')\n",
    "            fields_to_remove_exo.remove('GT')\n",
    "\n",
    "            fields_to_remove_imp = list(self.imp.entry.dtype.fields)\n",
    "            fields_to_remove_imp.append('info')\n",
    "            fields_to_remove_imp.remove('GT')\n",
    "            self.exo = self.exo.drop(*fields_to_remove_exo)\n",
    "            self.imp = self.imp.drop(*fields_to_remove_imp)\n",
    "        \n",
    "        if impute_nonsimilar_samples:\n",
    "            samples_in_imp = set(self.imp.s.collect())\n",
    "            samples_in_exo = set(self.exo.s.collect())\n",
    "            non_intersecting_samples = samples_in_imp.symmetric_difference(samples_in_exo)\n",
    "\n",
    "            for sample in non_intersecting_samples:\n",
    "                self.imp = self.imp.annotate_entries(**{sample: hl.missing(hl.tcall)})\n",
    "\n",
    "            joined_dataset = self.imp.union_rows(self.exo)\n",
    "\n",
    "            imputed_gt_value = hl.call(0, 0)\n",
    "            joined_dataset = joined_dataset.annotate_entries(**{sample: hl.coalesce(joined_dataset[sample], imputed_gt_value) for sample in non_intersecting_samples})\n",
    "\n",
    "        self._num_exo_sites, self._num_exo_samples = self.exo.count()\n",
    "        self._num_imp_sites, self._num_imp_samples = self.imp.count()\n",
    "        \n",
    "        _msg += f'Loaded {self._num_exo_sites} sites and {self._num_exo_samples} samples from {self.exo_path}.\\n'\n",
    "        _msg += f'Loaded {self._num_imp_sites} sites and {self._num_imp_samples} samples from {self.imp_path}.\\n'\n",
    "        print(_msg)\n",
    "        \n",
    "        if self._num_exo_samples != self._num_imp_samples:\n",
    "            samples_diff = list(set(self.exo.s.collect()) - set(self.imp.s.collect()))\n",
    "            diff_path = f'{self.out_path_prefix}no_sample_match.txt'\n",
    "            self.export_flat(samples_diff, diff_path)\n",
    "            _msg += f'Warning: {len(samples_diff)} have no match! Saving to {diff_path}.\\n'\n",
    "            print(f'Warning: {len(samples_diff)} have no match! Saving to {diff_path}.')\n",
    "        else:\n",
    "            _msg += 'Success! All samples matched.\\n'\n",
    "            print('Success! All samples matched.')\n",
    "                                   \n",
    "        # takes union of rows (sites), priority on exomes by default.\n",
    "        _msg += 'Merging...\\n'\n",
    "        print('Merging...')\n",
    "        \n",
    "        self.merge()\n",
    "        \n",
    "        #self.merged = self.merged.repartition(1000)\n",
    "        _msg += f'Done merging! Priority has been given to {self.exo_path}.\\nTherefore {self.exo_path} variant calls will be preserved and {self.imp_path} \\\n",
    "        will be overwritten.\\n'\n",
    "        print(f'Done merging! Priority has been given to {self.exo_path}.\\nTherefore {self.exo_path} variant calls will be preserved and {self.imp_path} will be overwritten.')\n",
    "\n",
    "        # also calculates overlap matrix: overlap on both variants and samples\n",
    "        self.calc_overlap(output_name=None)\n",
    "        \n",
    "        self.update_required = True\n",
    "        self.exome_split = None\n",
    "        self._multi_shape = None\n",
    "        self.PRIORITIES = {'exome', 'imputation'}\n",
    "        self.OUT_TYPES = {'vcf', 'PLINK', 'plink', 'hail'}\n",
    "        self._shape = None\n",
    "        self.samples = None\n",
    "        self.sample_conc = None\n",
    "        self.site_conc = None\n",
    "        self.concordance_results = None\n",
    "\n",
    "        _msg += f'Merged for a total of {self.shape[0]} sites and {self.shape[1]} samples.'\n",
    "        print(f'Merged for a total of {self.shape[0]} sites and {self.shape[1]} samples.')\n",
    "        \n",
    "        self.log += _msg\n",
    "    \n",
    "    def compute_nonref_concordance(self, matrix):\n",
    "        \"\"\"\n",
    "        Compute non-reference concordance from a concordance matrix.\n",
    "        \n",
    "        Args:\n",
    "            matrix (list): A 5x5 concordance matrix.\n",
    "            \n",
    "        Returns:\n",
    "            float: The non-reference concordance rate.\n",
    "        \"\"\"\n",
    "        submatrix = [row[2:] for row in matrix[2:]]\n",
    "        b, c = submatrix[0][1], submatrix[0][2]\n",
    "        d, e, f = submatrix[1][0], submatrix[1][1], submatrix[1][2]\n",
    "        g, h, i = submatrix[2][0], submatrix[2][1], submatrix[2][2]\n",
    "        numerator = e + i\n",
    "        denominator = (c + g) * 2 + b + d + e + f + h + i\n",
    "        return numerator / denominator if denominator > 0 else 0\n",
    "\n",
    "    def compute_concordance_metrics(self, matrix):\n",
    "        \"\"\"\n",
    "        Compute various concordance metrics from a concordance matrix.\n",
    "        \n",
    "        Args:\n",
    "            matrix (list): A 5x5 concordance matrix.\n",
    "            \n",
    "        Returns:\n",
    "            dict: A dictionary containing concordance metrics.\n",
    "        \"\"\"\n",
    "        genotype_rows = matrix[2:5] \n",
    "        n_concordant = sum(matrix[i][i] for i in range(2, 5))\n",
    "        \n",
    "        total_obs = sum(sum(row[2:5]) for row in genotype_rows)\n",
    "        n_discordant = total_obs - n_concordant\n",
    "        concordance_rate = n_concordant / total_obs if total_obs > 0 else 0\n",
    "        \n",
    "        nonref_concordance = self.compute_nonref_concordance(matrix)\n",
    "        het_to_ref = matrix[3][2]  \n",
    "        het_to_het = matrix[3][3] \n",
    "        \n",
    "        return {\n",
    "            'n_discordant': n_discordant,\n",
    "            'n_total': total_obs,\n",
    "            'concordance_rate': concordance_rate,\n",
    "            'nonref_concordance': nonref_concordance,\n",
    "            'het_to_ref': het_to_ref,\n",
    "            'het_to_het': het_to_het\n",
    "        }\n",
    "        \n",
    "    def load_sets(self, exo_path, imp_path):\n",
    "        \"\"\"\n",
    "        Loads exome and imputation datasets into hail format.\n",
    "        Args:\n",
    "            exo_path (str): Path to the exome variant call file in VCF format.\n",
    "            imp_path (str): Path to the imputation variant call file in VCF format.\n",
    "        \"\"\"\n",
    "        if exo_path[-4:] == '.bgz' or exo_path[-3:] == '.gz':\n",
    "            self.exo = hl.import_vcf(exo_path, \n",
    "                           force_bgz=True,\n",
    "                           call_fields=['LGT'])\n",
    "        else:\n",
    "            self.exo = hl.import_vcf(exo_path, \n",
    "                           call_fields=['LGT'])\n",
    "        \n",
    "        if imp_path[-4:] == '.bgz' or imp_path[-3:] == '.gz':\n",
    "            self.imp = hl.import_vcf(imp_path, \n",
    "                               force_bgz=True,)\n",
    "        else:\n",
    "            self.imp = hl.import_vcf(imp_path)\n",
    "    \n",
    "    def set_priority(self, new):\n",
    "        \"\"\"\n",
    "        sets a new priority for the dataset\n",
    "        Args:\n",
    "            new (str): New priority ('exome' or 'imputation').\n",
    "        \"\"\"\n",
    "        if not new in self.PRIORITIES:\n",
    "            raise ValueError(f'New priority must be one of {self.PRIORITIES}.')\n",
    "        self.priority = new\n",
    "    \n",
    "    def describe(self):\n",
    "        \"\"\"\n",
    "        Calls describe on the merged mt.\n",
    "        \"\"\"\n",
    "        self.merged.describe()\n",
    "        \n",
    "    def export_table(self, out_type, file_name_prefix='merged', overwrite=False):\n",
    "        \"\"\"\n",
    "        exports the merged dataset in the specified format.\n",
    "        Args:\n",
    "            out_type (str): Output format ('vcf', 'PLINK', 'plink', 'hail').\n",
    "            overwrite (bool, optional): If True, overwrite existing output files.\n",
    "        \"\"\"\n",
    "        if not out_type in self.OUT_TYPES:\n",
    "            raise ValueError(f'Export type must be in {self.OUT_TYPES}.')\n",
    "        if out_type == 'hail':\n",
    "            export_outpath = f'{self.out_path_prefix}{file_name_prefix}.mt'\n",
    "            self.merged.write(export_outpath)\n",
    "        elif out_type == 'vcf':\n",
    "            export_outpath = f'{self.out_path_prefix}{file_name_prefix}.vcf.bgz'\n",
    "            hl.export_vcf(self.merged, export_outpath)\n",
    "        else:\n",
    "            export_outpath = f'{self.out_path_prefix}{file_name_prefix}'\n",
    "            hl.export_plink(self.merged, export_outpath, ind_id=self.merged.s)\n",
    "            sites = self.generate_sites_file(export=True)\n",
    "            \n",
    "        _msg = f'Exported merged set to {export_outpath} with {self.shape[0]} sites and {self.shape[1]} samples.'\n",
    "        print(_msg)\n",
    "        \n",
    "        self.log += _msg + '\\n'\n",
    "            \n",
    "    def export_flat(self, itm, fname):\n",
    "        \"\"\"\n",
    "        exports a list or string to a flat file. NOTE: requires GCS connector.\n",
    "        Args:\n",
    "            itm (obj): Item to be exported.\n",
    "            fname (str): File name for the exported list.\n",
    "        \"\"\"\n",
    "        if isinstance(itm, list):\n",
    "            pd.DataFrame(itm).to_csv(fname, sep='\\t', index=False, header=False)\n",
    "        else:\n",
    "            with hadoop_open(fname, 'w') as f: \n",
    "                f.write(itm)\n",
    "            \n",
    "    def export_samples(self, sample_file_prefix='jc_samples'):\n",
    "        \"\"\"\n",
    "        exports the list of matching samples to a file.\n",
    "        Args:\n",
    "            sample_file_prefix (str, optional): Prefix for the sample file name.\n",
    "        \"\"\"\n",
    "        if not self.samples: \n",
    "            self.samples = self.merged.s.collect()\n",
    "        sample_out_path = f'{self.out_path_prefix}{sample_file_prefix}.txt'\n",
    "        self.export_flat(self.samples, sample_out_path)\n",
    "        \n",
    "        _msg = f'Matching sample list saved to {sample_out_path}.'\n",
    "        print(_msg)\n",
    "        self.log += _msg + '\\n'\n",
    "            \n",
    "    def split_multi(self):\n",
    "        \"\"\"\n",
    "        splits multi-allelic variants and updates the internal dataset [self.exome_split].\n",
    "        \"\"\"\n",
    "        if not self.exome_split:\n",
    "            bi = self.exo.filter_rows(hl.len(self.exo.alleles) == 2)\n",
    "            bi = bi.annotate_rows(a_index=1, was_split=False, old_locus=bi.locus, old_alleles=bi.alleles)\n",
    "            multi = self.exo.filter_rows(hl.len(self.exo.alleles) > 2)\n",
    "            split = hl.split_multi(multi)\n",
    "            split = split.union_rows(bi).drop(*['a_index', 'was_split', 'old_locus', 'old_alleles'])\n",
    "            self.exome_split = self.merged.union_rows(split)\n",
    "        else:\n",
    "            _msg = 'JointCalledSet has already been split!'\n",
    "            print(_msg)\n",
    "            self.log += _msg + '\\n'\n",
    "            \n",
    "        if not self._multi_shape:\n",
    "            self.count_multi_split()\n",
    "            \n",
    "        _msg = f'Before multi-allelic split, exome has {self._num_exo_sites} variants and {self._num_exo_samples} samples.'\n",
    "        print(_msg)\n",
    "        self.log += _msg + '\\n'\n",
    "        \n",
    "        _msg = f'After multi-allelic split, exome has {self._multi_shape[0]} variants and {self._multi_shape[1]} samples.'\n",
    "        print(_msg)\n",
    "        self.log += _msg + '\\n'\n",
    "        \n",
    "    def count_multi_split(self):\n",
    "        self._multi_shape = self.exome_split.count()\n",
    "\n",
    "    def concordance(self, output_name=None, filtered=False, output_json=True, graph_out=False):\n",
    "        \"\"\"\n",
    "        Calculates concordance between exome and imputation datasets, with detailed analysis across \n",
    "        allele count (AC) and minor allele frequency (MAF) bins.\n",
    "        \n",
    "        Args:\n",
    "            output_name (str or list, optional): Output file name or list of output file names.\n",
    "            filtered (bool, optional): If True, assumes datasets are already filtered. Defaults to False.\n",
    "            output_json (bool, optional): If True, outputs results to a JSON file. Defaults to True.\n",
    "            graph_out (bool, optional): If True, generates histograms. Defaults to False.\n",
    "        \n",
    "        Returns:\n",
    "            dict: A dictionary containing concordance results.\n",
    "        \"\"\"\n",
    "        _msg = 'Calculating concordance...\\n'\n",
    "        print(_msg)\n",
    "        self.log += _msg + '\\n'\n",
    "        \n",
    "        if not filtered:\n",
    "            # Filter to bi-allelic sites\n",
    "            exome_filtered = self.exo.filter_rows(hl.len(self.exo.alleles) == 2)\n",
    "            imputation_filtered = self.imp.filter_rows(hl.len(self.imp.alleles) == 2)\n",
    "            \n",
    "            n_samples = exome_filtered.count_cols()\n",
    "            _msg = f\"Total samples: {n_samples}\"\n",
    "            print(_msg)\n",
    "            self.log += _msg + '\\n'\n",
    "            \n",
    "            # Annotate with AC and MAF\n",
    "            exome_filtered = exome_filtered.annotate_rows(\n",
    "                AC=hl.agg.sum(exome_filtered.GT.n_alt_alleles()),\n",
    "                MAF=hl.agg.sum(exome_filtered.GT.n_alt_alleles()) / (2 * n_samples)\n",
    "            )\n",
    "            \n",
    "            imputation_filtered = imputation_filtered.annotate_rows(\n",
    "                AC=hl.agg.sum(imputation_filtered.GT.n_alt_alleles()),\n",
    "                MAF=hl.agg.sum(imputation_filtered.GT.n_alt_alleles()) / (2 * n_samples)\n",
    "            )\n",
    "            \n",
    "            # Find intersection of variants\n",
    "            exome_variants = exome_filtered.rows().select()._key_by_assert_sorted('locus', 'alleles')\n",
    "            imputation_variants = imputation_filtered.rows().select()._key_by_assert_sorted('locus', 'alleles')\n",
    "            \n",
    "            intersection = exome_variants.semi_join(imputation_variants)\n",
    "            \n",
    "            intersecting_variant_count = intersection.count()\n",
    "            _msg = f\"Intersecting variants: {intersecting_variant_count}\"\n",
    "            print(_msg)\n",
    "            self.log += _msg + '\\n'\n",
    "            \n",
    "            # Filter to intersecting variants\n",
    "            exome_filtered = exome_filtered.filter_rows(hl.is_defined(intersection[exome_filtered.row_key]))\n",
    "            imputation_filtered = imputation_filtered.filter_rows(hl.is_defined(intersection[imputation_filtered.row_key]))\n",
    "            \n",
    "            total_intersecting_genotypes = intersecting_variant_count * n_samples\n",
    "            _msg = f\"Total intersecting genotypes: {total_intersecting_genotypes}\"\n",
    "            print(_msg)\n",
    "            self.log += _msg + '\\n'\n",
    "            \n",
    "            # Annotate with AC and MAF bins\n",
    "            exome_filtered = exome_filtered.annotate_rows(\n",
    "                ac_bin=hl.case()\n",
    "                    .when(exome_filtered.AC <= 1, \"1\")\n",
    "                    .when(exome_filtered.AC <= 2, \"2\")\n",
    "                    .when(exome_filtered.AC <= 3, \"3\")\n",
    "                    .when(exome_filtered.AC <= 4, \"4\")\n",
    "                    .when(exome_filtered.AC <= 5, \"5\")\n",
    "                    .when(exome_filtered.AC <= 10, \"6-10\")\n",
    "                    .default(\"10+\"),\n",
    "                maf_bin=hl.case()\n",
    "                    .when(exome_filtered.MAF <= 0.02, \"1-2%\")\n",
    "                    .when(exome_filtered.MAF <= 0.05, \"2-5%\")\n",
    "                    .default(\"5%+\")\n",
    "            )\n",
    "            \n",
    "            imputation_filtered = imputation_filtered.annotate_rows(\n",
    "                ac_bin=hl.case()\n",
    "                    .when(imputation_filtered.AC <= 1, \"1\")\n",
    "                    .when(imputation_filtered.AC <= 2, \"2\")\n",
    "                    .when(imputation_filtered.AC <= 3, \"3\")\n",
    "                    .when(imputation_filtered.AC <= 4, \"4\")\n",
    "                    .when(imputation_filtered.AC <= 5, \"5\")\n",
    "                    .when(imputation_filtered.AC <= 10, \"6-10\")\n",
    "                    .default(\"10+\"),\n",
    "                maf_bin=hl.case()\n",
    "                    .when(imputation_filtered.MAF <= 0.02, \"1-2%\")\n",
    "                    .when(imputation_filtered.MAF <= 0.05, \"2-5%\")\n",
    "                    .default(\"5%+\")\n",
    "            )\n",
    "        else:\n",
    "            exome_filtered = self.exo\n",
    "            imputation_filtered = self.imp\n",
    "            n_samples = exome_filtered.count_cols()\n",
    "            intersecting_variant_count = exome_filtered.count_rows()\n",
    "            total_intersecting_genotypes = intersecting_variant_count * n_samples\n",
    "        \n",
    "        _msg = \"Computing global concordance\"\n",
    "        print(_msg)\n",
    "        self.log += _msg + '\\n'\n",
    "        \n",
    "        global_conc, cols_conc, rows_conc = hl.concordance(exome_filtered, imputation_filtered)\n",
    "        \n",
    "        variant_concordance = rows_conc.select(rows_conc.concordance)\n",
    "        \n",
    "        # Get AC and MAF bins\n",
    "        try:\n",
    "            exome_bins = exome_filtered.rows().select('ac_bin', 'maf_bin')\n",
    "            imputation_bins = imputation_filtered.rows().select('ac_bin', 'maf_bin')\n",
    "        except Exception as e:\n",
    "            _msg = f\"Error getting AC and MAF bins: {str(e)}\\nSkipping bin analysis.\"\n",
    "            print(_msg)\n",
    "            self.log += _msg + '\\n'\n",
    "            exome_bins = None\n",
    "            imputation_bins = None\n",
    "        \n",
    "        ac_bins = [\"1\", \"2\", \"3\", \"4\", \"5\", \"6-10\", \"10+\"]\n",
    "        maf_bins = [\"1-2%\", \"2-5%\", \"5%+\"]\n",
    "        \n",
    "        # Initialize result lists\n",
    "        ac_results = []\n",
    "        ac_results_imputation = []\n",
    "        maf_results = []\n",
    "        maf_results_imputation = []\n",
    "        \n",
    "        # Only perform bin analysis if bins are available\n",
    "        if exome_bins is not None and imputation_bins is not None:\n",
    "            # Process AC bins for exome\n",
    "            for ac_bin in ac_bins:\n",
    "                _msg = f\"Processing AC bin {ac_bin} (exome)\"\n",
    "                print(_msg)\n",
    "                self.log += _msg + '\\n'\n",
    "                \n",
    "                bin_variants = exome_bins.filter(exome_bins.ac_bin == ac_bin)\n",
    "                n_variants = bin_variants.count()\n",
    "                \n",
    "                if n_variants == 0:\n",
    "                    _msg = f\"No variants in AC bin {ac_bin} (exome)\"\n",
    "                    print(_msg)\n",
    "                    self.log += _msg + '\\n'\n",
    "                    empty_conc = [[0 for _ in range(5)] for _ in range(5)]\n",
    "                    ac_results.append({\n",
    "                        'bin': ac_bin,\n",
    "                        'concordance_matrix': empty_conc,\n",
    "                        'n_variants': 0,\n",
    "                        'het_genotypes_count': 0,\n",
    "                        **self.compute_concordance_metrics(empty_conc)\n",
    "                    })\n",
    "                    continue\n",
    "                \n",
    "                het_count = exome_filtered.filter_rows(exome_filtered.ac_bin == ac_bin).aggregate_entries(\n",
    "                    hl.agg.count_where(exome_filtered.GT.is_het())\n",
    "                )\n",
    "                \n",
    "                bin_concordance = variant_concordance.semi_join(bin_variants)\n",
    "                \n",
    "                bin_conc_matrix = [[0 for _ in range(5)] for _ in range(5)]\n",
    "                \n",
    "                for i in range(5):\n",
    "                    for j in range(5):\n",
    "                        bin_conc_matrix[i][j] = bin_concordance.aggregate(\n",
    "                            hl.agg.sum(bin_concordance.concordance[i][j])\n",
    "                        )\n",
    "                \n",
    "                metrics = self.compute_concordance_metrics(bin_conc_matrix)\n",
    "                \n",
    "                ac_results.append({\n",
    "                    'bin': ac_bin,\n",
    "                    'concordance_matrix': bin_conc_matrix,\n",
    "                    'n_variants': n_variants,\n",
    "                    'het_genotypes_count': het_count,\n",
    "                    **metrics\n",
    "                })\n",
    "            \n",
    "            # Process AC bins for imputation\n",
    "            for ac_bin in ac_bins:\n",
    "                _msg = f\"Processing AC bin {ac_bin} (imputation)\"\n",
    "                print(_msg)\n",
    "                self.log += _msg + '\\n'\n",
    "                \n",
    "                bin_variants = imputation_bins.filter(imputation_bins.ac_bin == ac_bin)\n",
    "                n_variants = bin_variants.count()\n",
    "                \n",
    "                if n_variants == 0:\n",
    "                    _msg = f\"No variants in AC bin {ac_bin} (imputation)\"\n",
    "                    print(_msg)\n",
    "                    self.log += _msg + '\\n'\n",
    "                    empty_conc = [[0 for _ in range(5)] for _ in range(5)]\n",
    "                    ac_results_imputation.append({\n",
    "                        'bin': ac_bin,\n",
    "                        'concordance_matrix': empty_conc,\n",
    "                        'n_variants': 0,\n",
    "                        'het_genotypes_count': 0,\n",
    "                        **self.compute_concordance_metrics(empty_conc)\n",
    "                    })\n",
    "                    continue\n",
    "                \n",
    "                het_count = imputation_filtered.filter_rows(imputation_filtered.ac_bin == ac_bin).aggregate_entries(\n",
    "                    hl.agg.count_where(imputation_filtered.GT.is_het())\n",
    "                )\n",
    "                \n",
    "                bin_concordance = variant_concordance.semi_join(bin_variants)\n",
    "                \n",
    "                bin_conc_matrix = [[0 for _ in range(5)] for _ in range(5)]\n",
    "                \n",
    "                for i in range(5):\n",
    "                    for j in range(5):\n",
    "                        bin_conc_matrix[i][j] = bin_concordance.aggregate(\n",
    "                            hl.agg.sum(bin_concordance.concordance[i][j])\n",
    "                        )\n",
    "                \n",
    "                metrics = self.compute_concordance_metrics(bin_conc_matrix)\n",
    "                \n",
    "                ac_results_imputation.append({\n",
    "                    'bin': ac_bin,\n",
    "                    'concordance_matrix': bin_conc_matrix,\n",
    "                    'n_variants': n_variants,\n",
    "                    'het_genotypes_count': het_count,\n",
    "                    **metrics\n",
    "                })\n",
    "            \n",
    "            # Process MAF bins for exome\n",
    "            for maf_bin in maf_bins:\n",
    "                _msg = f\"Processing MAF bin {maf_bin} (exome)\"\n",
    "                print(_msg)\n",
    "                self.log += _msg + '\\n'\n",
    "                \n",
    "                bin_variants = exome_bins.filter(exome_bins.maf_bin == maf_bin)\n",
    "                n_variants = bin_variants.count()\n",
    "                \n",
    "                if n_variants == 0:\n",
    "                    _msg = f\"No variants in MAF bin {maf_bin} (exome)\"\n",
    "                    print(_msg)\n",
    "                    self.log += _msg + '\\n'\n",
    "                    empty_conc = [[0 for _ in range(5)] for _ in range(5)]\n",
    "                    maf_results.append({\n",
    "                        'bin': maf_bin,\n",
    "                        'concordance_matrix': empty_conc,\n",
    "                        'n_variants': 0,\n",
    "                        'het_genotypes_count': 0,\n",
    "                        **self.compute_concordance_metrics(empty_conc)\n",
    "                    })\n",
    "                    continue\n",
    "                \n",
    "                het_count = exome_filtered.filter_rows(exome_filtered.maf_bin == maf_bin).aggregate_entries(\n",
    "                    hl.agg.count_where(exome_filtered.GT.is_het())\n",
    "                )\n",
    "                \n",
    "                bin_concordance = variant_concordance.semi_join(bin_variants)\n",
    "                \n",
    "                bin_conc_matrix = [[0 for _ in range(5)] for _ in range(5)]\n",
    "                \n",
    "                for i in range(5):\n",
    "                    for j in range(5):\n",
    "                        bin_conc_matrix[i][j] = bin_concordance.aggregate(\n",
    "                            hl.agg.sum(bin_concordance.concordance[i][j])\n",
    "                        )\n",
    "                \n",
    "                metrics = self.compute_concordance_metrics(bin_conc_matrix)\n",
    "                \n",
    "                maf_results.append({\n",
    "                    'bin': maf_bin,\n",
    "                    'concordance_matrix': bin_conc_matrix,\n",
    "                    'n_variants': n_variants,\n",
    "                    'het_genotypes_count': het_count,\n",
    "                    **metrics\n",
    "                })\n",
    "            \n",
    "            # Process MAF bins for imputation\n",
    "            for maf_bin in maf_bins:\n",
    "                _msg = f\"Processing MAF bin {maf_bin} (imputation)\"\n",
    "                print(_msg)\n",
    "                self.log += _msg + '\\n'\n",
    "                \n",
    "                bin_variants = imputation_bins.filter(imputation_bins.maf_bin == maf_bin)\n",
    "                n_variants = bin_variants.count()\n",
    "                \n",
    "                if n_variants == 0:\n",
    "                    _msg = f\"No variants in MAF bin {maf_bin} (imputation)\"\n",
    "                    print(_msg)\n",
    "                    self.log += _msg + '\\n'\n",
    "                    empty_conc = [[0 for _ in range(5)] for _ in range(5)]\n",
    "                    maf_results_imputation.append({\n",
    "                        'bin': maf_bin,\n",
    "                        'concordance_matrix': empty_conc,\n",
    "                        'n_variants': 0,\n",
    "                        'het_genotypes_count': 0,\n",
    "                        **self.compute_concordance_metrics(empty_conc)\n",
    "                    })\n",
    "                    continue\n",
    "                \n",
    "                het_count = imputation_filtered.filter_rows(imputation_filtered.maf_bin == maf_bin).aggregate_entries(\n",
    "                    hl.agg.count_where(imputation_filtered.GT.is_het())\n",
    "                )\n",
    "                bin_concordance = variant_concordance.semi_join(bin_variants)\n",
    "                bin_conc_matrix = [[0 for _ in range(5)] for _ in range(5)]\n",
    "                \n",
    "                for i in range(5):\n",
    "                    for j in range(5):\n",
    "                        bin_conc_matrix[i][j] = bin_concordance.aggregate(\n",
    "                            hl.agg.sum(bin_concordance.concordance[i][j])\n",
    "                        )\n",
    "                \n",
    "                metrics = self.compute_concordance_metrics(bin_conc_matrix)\n",
    "                \n",
    "                maf_results_imputation.append({\n",
    "                    'bin': maf_bin,\n",
    "                    'concordance_matrix': bin_conc_matrix,\n",
    "                    'n_variants': n_variants,\n",
    "                    'het_genotypes_count': het_count,\n",
    "                    **metrics\n",
    "                })\n",
    "        \n",
    "        # Compile results\n",
    "        results = {\n",
    "            'ac_results': ac_results,\n",
    "            'ac_results_imputation': ac_results_imputation,\n",
    "            'maf_results': maf_results,\n",
    "            'maf_results_imputation': maf_results_imputation,\n",
    "            'global_concordance': global_conc,\n",
    "            'n_samples': n_samples,\n",
    "            'intersecting_variant_count': intersecting_variant_count,\n",
    "            'total_intersecting_genotypes': total_intersecting_genotypes\n",
    "        }\n",
    "        \n",
    "        # Output results to JSON if requested\n",
    "        if output_json:\n",
    "            try:\n",
    "                json_out_path = f'{self.out_path_prefix}concordance_results.json'\n",
    "                with hl.hadoop_open(json_out_path, 'w') as f:\n",
    "                    json.dump(results, f)\n",
    "                _msg = f\"Saved concordance results to {json_out_path}\"\n",
    "                print(_msg)\n",
    "                self.log += _msg + '\\n'\n",
    "            except Exception as e:\n",
    "                _msg = f\"Error saving JSON results: {str(e)}\"\n",
    "                print(_msg)\n",
    "                self.log += _msg + '\\n'\n",
    "        \n",
    "        # Export TSV files if output_name is provided (backward compatibility)\n",
    "        if output_name:\n",
    "            try:\n",
    "                if isinstance(output_name, list):\n",
    "                    col_conc_fname = f'{self.out_path_prefix}{output_name[0]}'\n",
    "                    cols_conc.export(col_conc_fname)\n",
    "                    row_conc_fname = f'{self.out_path_prefix}{output_name[1]}'\n",
    "                    rows_conc.export(row_conc_fname)\n",
    "                elif isinstance(output_name, str):\n",
    "                    col_conc_fname = f'{self.out_path_prefix}{output_name}_sample.tsv'\n",
    "                    cols_conc.export(col_conc_fname)\n",
    "                    row_conc_fname = f'{self.out_path_prefix}{output_name}_sites.tsv'\n",
    "                    rows_conc.export(row_conc_fname)\n",
    "                else:\n",
    "                    raise ValueError(\"output_name must be either string or list.\")\n",
    "                    \n",
    "                _msg = f\"Saved sample and site concordance tables to {col_conc_fname} and {row_conc_fname}.\"\n",
    "                print(_msg)\n",
    "                self.log += _msg + '\\n'\n",
    "                \n",
    "                # Store for backward compatibility\n",
    "                self.gc = global_conc\n",
    "                self.sample_conc = pd.read_csv(col_conc_fname, sep='\\t', header=0)\n",
    "                self.site_conc = pd.read_csv(row_conc_fname, sep='\\t', header=0)\n",
    "                \n",
    "                # Generate histograms if requested\n",
    "                if graph_out:\n",
    "                    try:\n",
    "                        self.graph_histogram(self.sample_conc['n_discordant'], 'sample')\n",
    "                        self.graph_histogram(self.site_conc['n_discordant'], 'variant')\n",
    "                    except Exception as e:\n",
    "                        _msg = f\"Error generating histograms: {str(e)}\"\n",
    "                        print(_msg)\n",
    "                        self.log += _msg + '\\n'\n",
    "            except Exception as e:\n",
    "                _msg = f\"Error exporting concordance tables: {str(e)}\"\n",
    "                print(_msg)\n",
    "                self.log += _msg + '\\n'\n",
    "        \n",
    "        # Update the class with results\n",
    "        self.concordance_results = results\n",
    "        \n",
    "        return results\n",
    "\n",
    "    def graph_histogram(self, data, discordance_type):\n",
    "        \"\"\"\n",
    "        Generate a histogram for concordance data.\n",
    "        \n",
    "        Args:\n",
    "            data (array-like): Data to plot.\n",
    "            discordance_type (str): Type of discordance ('sample' or 'variant').\n",
    "        \"\"\"\n",
    "        if discordance_type not in {'sample', 'variant'}:\n",
    "            raise ValueError(\"Please provide either sample or variant discordance.\")\n",
    "        \n",
    "        try:\n",
    "            hist, edges = np.histogram(data)\n",
    "            p = figure(title=f\"Distribution of {discordance_type} discordance\", \n",
    "                      x_axis_label=\"n_discordant\", \n",
    "                      y_axis_label=\"Frequency\")\n",
    "            p.quad(top=hist, bottom=0, left=edges[:-1], right=edges[1:], fill_color=\"navy\", line_color=\"white\", alpha=0.5)\n",
    "            show(p)\n",
    "        except Exception as e:\n",
    "            _msg = f\"Error generating histogram: {str(e)}\"\n",
    "            print(_msg)\n",
    "            self.log += _msg + '\\n'\n",
    "\n",
    "    def create_summary_dataframes(self, results=None):\n",
    "        \"\"\"\n",
    "        Create summary dataframes from concordance results.\n",
    "        \n",
    "        Args:\n",
    "            results (dict, optional): Concordance results. If None, uses self.concordance_results.\n",
    "            \n",
    "        Returns:\n",
    "            tuple: A tuple containing (ac_df, maf_df) - dataframes for AC and MAF results.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            if results is None:\n",
    "                if hasattr(self, 'concordance_results'):\n",
    "                    results = self.concordance_results\n",
    "                else:\n",
    "                    raise ValueError(\"No concordance results available. Run concordance() first.\")\n",
    "            \n",
    "            def process_results(results_list, source):\n",
    "                df = pd.DataFrame([{\n",
    "                    'bin': r['bin'],\n",
    "                    'concordance_rate': r['concordance_rate'],\n",
    "                    'nonref_concordance': r['nonref_concordance'],\n",
    "                    'n_variants': r['n_variants'],\n",
    "                    'n_discordant': r['n_discordant'],\n",
    "                    'n_total': r['n_total'],\n",
    "                    'het_to_ref': r['het_to_ref'],\n",
    "                    'het_to_het': r['het_to_het'],\n",
    "                    'het_genotypes_count': r['het_genotypes_count'],\n",
    "                    'source': source\n",
    "                } for r in results_list])\n",
    "                return df\n",
    "            \n",
    "            ac_df_exome = process_results(results['ac_results'], 'exome')\n",
    "            ac_df_imputation = process_results(results['ac_results_imputation'], 'imputation')\n",
    "            ac_df = pd.concat([ac_df_exome, ac_df_imputation])\n",
    "            \n",
    "            ac_df['bin'] = pd.Categorical(ac_df['bin'], categories=['1', '2', '3', '4', '5', '6-10', '10+'], ordered=True)\n",
    "            ac_df = ac_df.sort_values(['source', 'bin'])\n",
    "            \n",
    "            maf_df_exome = process_results(results['maf_results'], 'exome')\n",
    "            maf_df_imputation = process_results(results['maf_results_imputation'], 'imputation')\n",
    "            maf_df = pd.concat([maf_df_exome, maf_df_imputation])\n",
    "            \n",
    "            maf_df['bin'] = pd.Categorical(maf_df['bin'], categories=['1-2%', '2-5%', '5%+'], ordered=True)\n",
    "            maf_df = maf_df.sort_values(['source', 'bin'])\n",
    "            \n",
    "            return ac_df, maf_df\n",
    "        except Exception as e:\n",
    "            import traceback\n",
    "            _msg = f\"Error creating summary dataframes: {str(e)}\\n{traceback.format_exc()}\"\n",
    "            print(_msg)\n",
    "            self.log += _msg + '\\n'\n",
    "            return None, None\n",
    "\n",
    "    def display_result_tables(self, ac_df=None, maf_df=None, results=None):\n",
    "        \"\"\"\n",
    "        Display formatted concordance result tables.\n",
    "        \n",
    "        Args:\n",
    "            ac_df (DataFrame, optional): DataFrame with AC results.\n",
    "            maf_df (DataFrame, optional): DataFrame with MAF results.\n",
    "            results (dict, optional): Concordance results. Used if ac_df or maf_df is None.\n",
    "            \n",
    "        Returns:\n",
    "            tuple: A tuple containing (ac_table, maf_table) - formatted tables.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            if ac_df is None or maf_df is None:\n",
    "                if results is None:\n",
    "                    if hasattr(self, 'concordance_results'):\n",
    "                        results = self.concordance_results\n",
    "                    else:\n",
    "                        raise ValueError(\"No concordance results available. Run concordance() first.\")\n",
    "                ac_df, maf_df = self.create_summary_dataframes(results)\n",
    "            \n",
    "            def format_table(df, bin_name):\n",
    "                table = df.copy()\n",
    "                table['concordance_rate'] = table['concordance_rate'].map('{:.3%}'.format)\n",
    "                table['nonref_concordance'] = table['nonref_concordance'].map('{:.3%}'.format)\n",
    "                table = table.rename(columns={\n",
    "                    'bin': f'{bin_name} Bin',\n",
    "                    'n_variants': '# Intersecting Variants',\n",
    "                    'n_discordant': '# Discordant',\n",
    "                    'n_total': '# Total Genotypes',\n",
    "                    'concordance_rate': 'Concordance Rate',\n",
    "                    'nonref_concordance': 'Non-ref Concordance',\n",
    "                    'het_to_ref': 'Het→Ref',\n",
    "                    'het_to_het': 'Het→Het',\n",
    "                    'het_genotypes_count': 'Het Genotypes',  \n",
    "                    'source': 'Source'\n",
    "                })\n",
    "                return table\n",
    "            \n",
    "            ac_table = format_table(ac_df, 'AC')\n",
    "            _msg = \"Allele Count (AC) Results:\"\n",
    "            print(_msg)\n",
    "            self.log += _msg + '\\n'\n",
    "            \n",
    "            for source in ['exome', 'imputation']:\n",
    "                _msg = f\"\\n{source.upper()} AC Results:\"\n",
    "                print(_msg)\n",
    "                self.log += _msg + '\\n'\n",
    "                \n",
    "                source_table = ac_table[ac_table['Source'] == source].drop('Source', axis=1)\n",
    "                _msg = source_table.to_string(index=False)\n",
    "                print(_msg)\n",
    "                self.log += _msg + '\\n'\n",
    "            \n",
    "            maf_table = format_table(maf_df, 'MAF')\n",
    "            _msg = \"\\nMinor Allele Frequency (MAF) Results:\"\n",
    "            print(_msg)\n",
    "            self.log += _msg + '\\n'\n",
    "            \n",
    "            for source in ['exome', 'imputation']:\n",
    "                _msg = f\"\\n{source.upper()} MAF Results:\"\n",
    "                print(_msg)\n",
    "                self.log += _msg + '\\n'\n",
    "                \n",
    "                source_table = maf_table[maf_table['Source'] == source].drop('Source', axis=1)\n",
    "                _msg = source_table.to_string(index=False)\n",
    "                print(_msg)\n",
    "                self.log += _msg + '\\n'\n",
    "            \n",
    "            _msg = \"\\nSummary Statistics:\"\n",
    "            print(_msg)\n",
    "            self.log += _msg + '\\n'\n",
    "            \n",
    "            _msg = f\"Total Samples: {results['n_samples']}\"\n",
    "            print(_msg)\n",
    "            self.log += _msg + '\\n'\n",
    "            \n",
    "            _msg = f\"Total Intersecting Variants: {results['intersecting_variant_count']}\"\n",
    "            print(_msg)\n",
    "            self.log += _msg + '\\n'\n",
    "            \n",
    "            _msg = f\"Total Intersecting Genotypes: {results['total_intersecting_genotypes']}\"\n",
    "            print(_msg)\n",
    "            self.log += _msg + '\\n'\n",
    "            \n",
    "            _msg = f\"Genotypes per variant: {results['n_samples']}\"\n",
    "            print(_msg)\n",
    "            self.log += _msg + '\\n'\n",
    "            \n",
    "            return ac_table, maf_table\n",
    "        except Exception as e:\n",
    "            import traceback\n",
    "            _msg = f\"Error displaying result tables: {str(e)}\\n{traceback.format_exc()}\"\n",
    "            print(_msg)\n",
    "            self.log += _msg + '\\n'\n",
    "            return None, None\n",
    "\n",
    "    def save_concordance_results(self, file_path=None):\n",
    "        \"\"\"\n",
    "        Save concordance results to a JSON file.\n",
    "        \n",
    "        Args:\n",
    "            file_path (str, optional): Path to save results. If None, uses default path.\n",
    "            \n",
    "        Returns:\n",
    "            str: Path to the saved file, or None if there was an error.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            if not hasattr(self, 'concordance_results'):\n",
    "                raise ValueError(\"No concordance results available. Run concordance() first.\")\n",
    "            \n",
    "            if file_path is None:\n",
    "                file_path = f'{self.out_path_prefix}concordance_results.json'\n",
    "            \n",
    "            with hl.hadoop_open(file_path, 'w') as f:\n",
    "                json.dump(self.concordance_results, f)\n",
    "            \n",
    "            _msg = f\"Saved concordance results to {file_path}\"\n",
    "            print(_msg)\n",
    "            self.log += _msg + '\\n'\n",
    "            \n",
    "            return file_path\n",
    "        except Exception as e:\n",
    "            import traceback\n",
    "            _msg = f\"Error saving concordance results: {str(e)}\\n{traceback.format_exc()}\"\n",
    "            print(_msg)\n",
    "            self.log += _msg + '\\n'\n",
    "            return None\n",
    "\n",
    "    def load_concordance_results(self, file_path=None):\n",
    "        \"\"\"\n",
    "        Load concordance results from a JSON file.\n",
    "        \n",
    "        Args:\n",
    "            file_path (str, optional): Path to load results from. If None, uses default path.\n",
    "        \n",
    "        Returns:\n",
    "            dict: The loaded concordance results, or None if there was an error.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            if file_path is None:\n",
    "                file_path = f'{self.out_path_prefix}concordance_results.json'\n",
    "            \n",
    "            with hl.hadoop_open(file_path, 'r') as f:\n",
    "                results = json.load(f)\n",
    "            \n",
    "            self.concordance_results = results\n",
    "            \n",
    "            _msg = f\"Loaded concordance results from {file_path}\"\n",
    "            print(_msg)\n",
    "            self.log += _msg + '\\n'\n",
    "            \n",
    "            return results\n",
    "        except Exception as e:\n",
    "            import traceback\n",
    "            _msg = f\"Error loading concordance results: {str(e)}\\n{traceback.format_exc()}\"\n",
    "            print(_msg)\n",
    "            self.log += _msg + '\\n'\n",
    "            return None\n",
    "            \n",
    "    def remove_discordant(self, which, thresh=None):\n",
    "        \"\"\"\n",
    "        Filter the merged dataset to remove samples or variants with high discordance.\n",
    "        \n",
    "        Args:\n",
    "            which (str): Type of filtering ('sample', 'variant', or 'site').\n",
    "            thresh (int): Threshold for number of discordant genotypes to allow.\n",
    "        \"\"\"\n",
    "        if self.sample_conc is None or self.site_conc is None:\n",
    "            raise ValueError('Please compute concordance tables first!')\n",
    "        if not thresh:\n",
    "            raise ValueError('Please provide a threshold.')\n",
    "        if which not in set(['sample', 'variant', 'site']):\n",
    "            raise ValueError('Must be either variant, site, or sample discordance.')\n",
    "        \n",
    "        if which == 'sample':\n",
    "            df = self.sample_conc\n",
    "        else:\n",
    "            df = self.site_conc\n",
    "        filtered = df[df['n_discordant'] > thresh]\n",
    "        \n",
    "        if which == 'site' or which == 'variant':\n",
    "            set_keep = hl.literal(set(list(filtered['locus'].apply(lambda x: hl.parse_locus(x)))))\n",
    "            _msg = f'Filtering {which} to < {thresh} n_discordant.'\n",
    "            print(_msg)\n",
    "            self.log += _msg + '\\n'\n",
    "            self.merged = self.merged.filter_rows(set_keep.contains(self.merged.locus), keep=True)\n",
    "        else:\n",
    "            set_keep = hl.literal(set(filtered['s']))\n",
    "            _msg = f'Filtering {which} to < {thresh} n_discordant.'\n",
    "            print(_msg)\n",
    "            self.log += _msg + '\\n'\n",
    "            self.merged = self.merged.filter_cols(set_keep.contains(self.merged.s), keep=True)\n",
    "        \n",
    "    def calc_overlap(self, output_name=None):\n",
    "        \"\"\"\n",
    "        calculates the overlap matrices of variants and samples between datasets, first filtering to bi-allelic sites\n",
    "        Args:\n",
    "            output_name (str, optional): Output file name.\n",
    "        \"\"\"\n",
    "        self.overlap = self.exo.semi_join_rows(self.imp.rows())\n",
    "        self.overlap = self.overlap.semi_join_cols(self.exo.cols())\n",
    "        self._n_overlap_sites, self._n_overlap_samples = self.overlap.count()\n",
    "        print(f'{self._n_overlap_sites} sites and {self._n_overlap_samples} samples are similar between {self.exo_path} and {self.imp_path}.')\n",
    "        \n",
    "        if output_name:\n",
    "            out_name = f'{self.out_path_prefix}{output_name}.txt'\n",
    "            locs = self.overlap.locus.collect()\n",
    "            loci = list(map(lambda x: str(x), locs))\n",
    "            self.export_flat(loci, out_name)\n",
    "            print(f'{len(loci)} variants overlap between both files, written to {out_name}.')\n",
    "            \n",
    "    def plot_concordance(self, col_conc_fname):\n",
    "        \"\"\"\n",
    "        Plot concordance vs MAF.\n",
    "        \n",
    "        Args:\n",
    "            col_conc_fname (str): Path to column concordance file.\n",
    "        \"\"\"\n",
    "        self.merged = hl.variant_qc(self.merged)\n",
    "        self.merged = self.merged.annotate_rows(MAF=hl.min(self.merged.variant_qc.AF))\n",
    "        \n",
    "        col_conc = pd.read_csv(col_conc_fname, sep='\\t', header=0)\n",
    "        x = self.merged.MAF.collect()\n",
    "        y = (np.ones(len(col_conc['n_discordant'].values)) * self.shape[1] - col_conc['n_discordant'].values) / self.shape[1]        \n",
    "        \n",
    "        z = np.array(list(sorted(zip(x, y), key=lambda v: v[0]))).T\n",
    "        \n",
    "        p = figure(title=\"Concordance over all chromosomes\", \n",
    "                   x_axis_label='MAF (%)', \n",
    "                   y_axis_label='Sample concordance')\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "    \n",
    "        p.circle(z[0, :]*100, z[1, :], size=3, alpha=0.5)\n",
    "        show(p)\n",
    "        self.maf_plot = p\n",
    "\n",
    "    def generate_sites_file(self, export=False):\n",
    "        \"\"\"\n",
    "        Generate a sites file with marker information.\n",
    "        \n",
    "        Args:\n",
    "            export (bool, optional): If True, export the sites file.\n",
    "            \n",
    "        Returns:\n",
    "            str: Generated sites string.\n",
    "        \"\"\"\n",
    "        rsids = self.merged.rsid.collect()\n",
    "        locus = self.merged.locus.collect()\n",
    "        \n",
    "        sites = 'MarkerName\\tChromosome\\tPosition\\n'\n",
    "        for rsid, loc in zip(rsids, locus):\n",
    "            sites += f'{rsid}\\t{loc.contig}\\t{loc.position}\\n'\n",
    "            \n",
    "        if export:\n",
    "            export_outpath = f'{self.out_path_prefix}.sites'\n",
    "            self.export_flat(sites, export_outpath)\n",
    "            \n",
    "        return sites\n",
    "    \n",
    "    def merge(self):\n",
    "        \"\"\"\n",
    "        Merge exome and imputation datasets with priority.\n",
    "        \"\"\"\n",
    "        def align_mt2_cols_to_mt1(mt1, mt2):\n",
    "            mt1 = mt1.add_col_index()\n",
    "            mt2 = mt2.add_col_index()\n",
    "            new_col_order = mt2.index_cols(mt1.col_key).col_idx.collect()\n",
    "            return mt2.choose_cols(new_col_order)\n",
    "\n",
    "        if self.priority == 'exome':\n",
    "            not_exo = self.imp.anti_join_rows(self.exo.rows())\n",
    "            self.exo = self.exo.semi_join_cols(not_exo.cols())\n",
    "            not_exo = align_mt2_cols_to_mt1(self.exo, not_exo)\n",
    "            self.merged = self.exo.union_rows(not_exo)\n",
    "        else:\n",
    "            not_imp = self.exo.anti_join_rows(self.imp.rows())\n",
    "            self.imp = self.imp.semi_join_cols(not_imp.cols())\n",
    "            not_imp = align_mt2_cols_to_mt1(self.imp, not_imp)\n",
    "            self.merged = self.imp.union_rows(not_imp)\n",
    "    \n",
    "    def save_log(self):\n",
    "        \"\"\"\n",
    "        Saves log string to logging file.\n",
    "        \"\"\"\n",
    "        self.export_flat(self.log, self.log_outpath)\n",
    "    \n",
    "    @property\n",
    "    def shape(self):\n",
    "        if self._shape is None:\n",
    "            self._shape = self.merged.count()\n",
    "        return self._shape"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
